{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive sampling\n",
    "\n",
    "This script shows how to get geomtries out of the files produced with SchNarc using the adaptive sampling mode, an example to run the calculations with SHARC and parser to get the data out of SHARC-output files including a function to extend the data base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5291772105638411\n"
     ]
    }
   ],
   "source": [
    "path = \"./\"\n",
    "import ase.io\n",
    "import ase\n",
    "import numpy as np\n",
    "import schnetpack as spk\n",
    "from ase.units import Bohr\n",
    "print(Bohr)\n",
    "import os\n",
    "SHARC=\"/user/julia/software/SchNarc/sharc/source/../bin/\"\n",
    "from schnarc.utils import read_QMout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running adaptive sampling\n",
    "\n",
    "For adaptive sampling we need two models. These are provided in the folder \"Models\". \n",
    "\n",
    "For adaptive sampling you need 4 files:\n",
    "* geom\n",
    "* veloc\n",
    "* input\n",
    "* run.sh\n",
    "\n",
    "\n",
    "The geom, veloc and input files for the dynamics with SchNarc can be generated with SHARC. Please have a look at the SHARC tutorial on how to set up trajectories. You will have 10 folders already provided for the purpose of this tutorial that contain these files.\n",
    "The run.sh file contains the commands for adaptive sampling.\n",
    "You need at least two models to do adaptive sampling.\n",
    "\n",
    "Go into each of the folders and execute the run.sh file. It will terminate when the error of the models is larger than 1 eV.\n",
    "\n",
    "Alternatively, below you can see how to run a trajectory.\n",
    "You need the arguments --modelpaths PATH --adaptive --threhsolds 1 1 1 1 1.\n",
    "The latter are errors for energy, forces, dipoles, socs, and nacs in a.u. that should not be exceeded by the two models. PATH should point to the path of the second ML model.\n",
    "The dynamics will be run using the mean of all properties from the specified models. Note that you can use as many models as you like, just add more paths after the argument \"--modelpaths\".\n",
    "\n",
    "If you set --print_uncertainty, the deviation of the model predictions will be print in a.u. into the file \"NN.log\".\n",
    "Note that you can also use a gpu for dynamics, then please add \"--cuda\".\n",
    "\n",
    "#### Attention\n",
    "Running dynamics from this notebook takes much longer than executing the file \"run.sh\" in each folder using bash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SCHNARC = \"../../../src/scripts/\"\n",
    "os.system(\"cd TRAJ_1/ ; python %s/schnarc_md.py pred ../../DBs/Fulvene.db ../../Models/Model1 --modelpaths ../../Models/Model2 --adaptive --thresholds 0.1 100 100 100 1 --print_uncertainty >> NN.log 2>> NN.err \"%SCHNARC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First, use the data extractor from SHARC to generate the output.xyz files with the geometries.\n",
    "Read the geometries with ase.\n",
    "We have chosen to take the last and the last-5 geometry for the adaption of the training set\n",
    "\n",
    "## Get geometries \n",
    "### Trajectories from ML model trained on energies and forces (Hessian approximation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "geoms=[]\n",
    "ntraj = 1\n",
    "ind=[]\n",
    "for i in range(ntraj):\n",
    "    \n",
    "    #generate output.xyz \n",
    "    os.system(\"cd %s/TRAJ_%i/ ; %s/data_extractor_NetCDF.x -xyz output.dat; cd ../..\"%(path,i+1,SHARC))\n",
    "    \n",
    "    geometries = ase.io.read(\"%s/TRAJ_%i/output.xyz\"%(path,i+1),\":\")\n",
    "    \n",
    "    #this happens if the first geometry is predicted with a too large error\n",
    "    if len(geometries)==0:\n",
    "        if i+1 not in ind:\n",
    "            ind.append(i+1)\n",
    "    else:\n",
    "        #append geometries with last geometry of trajectory file\n",
    "        geoms.append(geometries[-1])\n",
    "print(len(geoms))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make folders for calculation\n",
    "\n",
    "In this example we will generate X folder (X=number of geometries for expanding the training set).\n",
    "We will copy all relevant files to make QM calculations with SHARC.\n",
    "Make sure you have downloaded the \"Inputfile\" folder and make sure to adapt the \"run.sh\" file for your cluster. \n",
    "For the sake of the tutorial, we will also provide the reference calculations in the zip-file \"AdaptiveSampling-Calculations.zip\". So you don't need to run the calculations, but you can copy the results and go on with adapting the training set.\n",
    "Note that this zip only contains all file in the first folder, and in all other folders only the inputs and outputs are saved.\n",
    "\n",
    "Note that you additionally have to copy the SAVEDIR folder and to adapt the \"QMstring\"-file to carry out phasecorrection as it was done for generating the initial training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make adaptive sampling folders\n",
    "inputs = \"/../Inputfiles/\"\n",
    "for i in range(len(geoms)):\n",
    "    os.system(\"mkdir QM\")\n",
    "    os.system(\"mkdir QM/Geom_%05d\"%i)\n",
    "    ase.io.write(\"QM/Geom_%05d/geometry.xyz\"%i,geoms[i])\n",
    "    os.system(\"cp QM/Geom_%05d/geometry.xyz QM/Geom_%05d/QM.in\"%(i,i))\n",
    "    os.system(\"cat %s/QMstring >> QM/Geom_%05d/QM.in\" %(inputs,i))\n",
    "    os.system(\" cp %s/MOLPRO* QM/Geom_%05d/\"%(inputs,i))\n",
    "    #os.system(\" mkdir QM/Geom_%05d/SAVEDIR\"%i)\n",
    "    #os.system(\" cp %s/wf.1 QM/Geom_%05d/SAVEDIR\"%(inputs,i))\n",
    "    os.system(\" cp %s/run.sh QM/Geom_%05d/\" %(inputs,i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy the submission script for the cluster you use into every folder and submit the calculations\n"
     ]
    }
   ],
   "source": [
    "print(\"Copy the submission script for the cluster you use into every folder and submit the calculations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapting the training set\n",
    "\n",
    "For now we copy a dummy-QMout file from the Initial training set generation into every QM-folder for demonstration.\n",
    "Note that units are all given in a.u. and that the geometries need to be saved in a.u. as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(geoms)):\n",
    "    os.system(\"cp ../1_TrainingSet/InitialConditions/ICOND_00001/QM.out QM/Geom_%05d/\"%i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read QMout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterating over all files\n",
    "nfiles = len(geoms)\n",
    "#define number of states\n",
    "ntriplets = 0\n",
    "nsinglets = 2\n",
    "nstates = nsinglets + 3 * ntriplets\n",
    "\n",
    "#define number of atoms\n",
    "natoms = 12\n",
    "#number of nacs\n",
    "nnacs = int(nsinglets*(nsinglets-1)/2) + int(ntriplets*(ntriplets-1)/2)\n",
    "#number of dipole moment values\n",
    "ndipoles = int(nsinglets+ntriplets+nnacs)\n",
    "\n",
    "\n",
    "#for conversion of atoms into bohr\n",
    "from ase.units import Bohr\n",
    "\n",
    "#data dictionary for updating the data base\n",
    "data = {}\n",
    "\n",
    "filename=\"./QM/Geom_00000/QM.out\"\n",
    "#we don't have spin-orbit couplings\n",
    "socs=False\n",
    "#read properties\n",
    "data=read_QMout(filename,natoms,socs,nsinglets,ntriplets,0.5)\n",
    "atoms = ase.io.read(\"TRAJ_1/output.xyz\",\"-1\")\n",
    "#convert to bohr\n",
    "from ase.units import Bohr\n",
    "atoms = ase.atoms.Atoms(atoms.get_atomic_numbers(),atoms.get_positions()/Bohr)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add data to the data base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the old data base\n",
    "from ase.db import connect\n",
    "old_db = connect(\"../DBs/Fulvene.db\")\n",
    "for i in range(len(data)):\n",
    "    old_db.write(atoms[i],data=data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have added 20 data points to the data set and now have a total number of 120 data points.\n"
     ]
    }
   ],
   "source": [
    "print(\"We have added\",len(atoms), \"data points to the data set and now have a total number of\",len(old_db), \"data points.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now retrain the models and redo the sampling until the training set is large enough.\n",
    "For fulvene and the 40 fs we compute, the training set is large enough.\n",
    "\n",
    "If you want to make production runs and use a second model to compare the energies, but not to compute forces or hessians, then use \" --emodel2 PATH/ \" instead of \"--adaptive --modelpaths PATH\".\n",
    "The threshold is set to 1 eV per default. You can change the threshold via \"--thresholds 1 1 1 1 1\". (thresholds are always given for every possible property)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
