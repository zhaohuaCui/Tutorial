{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate initial training set from SHARC-outputs\n",
    "\n",
    "This script shows how to obtain an initial training set starting from calculations carried out with SHARC using Wigner sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ase.io\n",
    "import ase\n",
    "import numpy as np\n",
    "import schnetpack as spk\n",
    "from ase.units import Bohr\n",
    "import os\n",
    "SHARC=\"/user/julia/software/SchNarc/sharc/source/../bin/\"\n",
    "#import sharc\n",
    "import schnarc\n",
    "#print(help(schnarc))\n",
    "from schnarc.utils import read_QMout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the QM outputs and generate the training set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that units are all given in a.u. and that the geometries need to be saved in a.u. as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#iterating over all files\n",
    "nfiles = 100\n",
    "#define number of states\n",
    "ntriplets = 0\n",
    "nsinglets = 2\n",
    "nstates = nsinglets + 3 * ntriplets\n",
    "\n",
    "#define number of atoms\n",
    "natoms = 12\n",
    "\n",
    "\n",
    "#read geometries --> these are in Angstr√∂m!\n",
    "geoms = ase.io.read(\"./InitialConditions/initconds.xyz\",\":\")\n",
    "\n",
    "#transform geoms into a.u.\n",
    "atoms=[]\n",
    "for i in range(len(geoms)):\n",
    "    atoms.append(ase.atoms.Atoms(geoms[i].get_atomic_numbers(),geoms[i].get_positions()/Bohr))\n",
    "\n",
    "\n",
    "#for conversion of atoms into bohr\n",
    "from ase.units import Bohr\n",
    "\n",
    "#data dictionary for updating the data base\n",
    "data = {}\n",
    "\n",
    "path=\"./InitialConditions/ICOND\"\n",
    "#we don't have spin-orbit couplings\n",
    "socs=False\n",
    "\n",
    "#read properties\n",
    "for ifile in range(nfiles):\n",
    "    filename = \"%s_%05d/QM.out\"%(path,ifile)\n",
    "    data[ifile]={}\n",
    "    #the last number is the threshold for overlaps. Do not change this value unless you know what it means and are clear about the consequences.\n",
    "    data[ifile]=read_QMout(filename,natoms,socs,nsinglets,ntriplets,0.5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Including phase correction\n",
    "\n",
    "If you wish to include phase correction, you need to have computed the overlap matrix between your chosen starting geometry and every other geometry you want to include in the training set.\n",
    "If you don't want to do phase correction, skip this section and go directly to the section \"Making the data set\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QM calculations: getting the phase vectors\n",
    "\n",
    "In case you have carried out overlap calculations, you will notice that there is an additional entry called \"phases\".\n",
    "If the phases are not realiable enough, this entry will be missing. You can iterate over the data to find out which data points require additional interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98 data points do not contain reliable phases.\n"
     ]
    }
   ],
   "source": [
    "phasesnotfound = []\n",
    "for i in range(len(data)):\n",
    "    if \"phases\" in data[i]:\n",
    "        pass\n",
    "    else:\n",
    "        #print(\"Folder \", i,\" requires interpolation to get phases.\")\n",
    "        phasesnotfound.append(i)\n",
    "print(len(phasesnotfound), \"data points do not contain reliable phases.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolation between geometries with insufficiently large overlaps\n",
    "\n",
    "As you can see, only a few calculations have sufficiently large overlaps with the chosen starting geometry. All these calculations require interpolation between the starting geometry and the desired geometry. An overlap calculation has to be carried out from the starting geometry to the geometry that should be included in the set.\n",
    "The phases for each calculation have to be found and multiplied with the phases of the previous geometry.\n",
    "\n",
    "You can find an example in the folder: Interpolation_Geometry1\n",
    "* start.xyz: geometry of the chosen reference point\n",
    "* end.xyz: geometry which should be included in the training set\n",
    "\n",
    "You can also give a number of interpolation steps, this is the last argument you add. In this case we have chosen 30 geometries interpolations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now execute the interpolation script\n",
    "\n",
    "from schnarc.utils import interpolate\n",
    "#from interpolation import interpolate\n",
    "start = \"start.xyz\"\n",
    "end = \"end.xyz\"\n",
    "interpolate(start,end,natoms,nsinglets,ntriplets,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpolate:\n",
    "After interpolation (script is provided in the schnarc-utils), you will receive several files named \"interpolateXX.xyz\". Each file is one step in the interpolation. all geometries will be saved as separate files named interpolateXX.xyz with XX being the index of ther interpolation step. Geometries can be visualized with e.g. molden. Remember the units are in Bohr.\n",
    "\n",
    "In this tutorial, all geometries are saved already in the folder \"Interpolation_Geometry1\". Each folder Calc_XXXXX corresponds to a folder of the interpolated structure. \n",
    "You can then carry out each calculation serially and compute the wave function overlaps. If you wish to include these data points in the training set, then compute all desired properties for each step. Note that the calculations cannot be done in parallel and need to be done after one another.\n",
    "\n",
    "You will already be provided with the folders including the computations carried out.\n",
    "\n",
    "We will now read in all properties and the phasevectors and skip the actual QM calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterating over all files\n",
    "nfiles = 30\n",
    "\n",
    "#read geometries\n",
    "geoms = ase.io.read(\"Interpolation_Geometry1/geometries.xyz\",\":\")\n",
    "\n",
    "#data dictionary for updating the data base\n",
    "datainterpolate = {}\n",
    "#TODO\n",
    "for i in range(nfiles):\n",
    "    filename = \"Interpolation_Geometry1/Calc_%05d/QM.out\"%i\n",
    "    \n",
    "    datainterpolate[i]=read_QMout(filename,natoms,socs,nsinglets,ntriplets,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data points could be corrected and data can be phase corrected.\n"
     ]
    }
   ],
   "source": [
    "phases_found = True \n",
    "for i in range(len(datainterpolate)):\n",
    "    if \"phases\" not in datainterpolate[i]:\n",
    "        print(\"Phase correction was not successful for interpolation step\", i)\n",
    "        phases_found= False\n",
    "if phases_found == True:\n",
    "    print(\"All data points could be corrected and data can be phase corrected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to multiply each phase vector to find the phase vector of the final geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_phase = np.ones(nsinglets+ntriplets)\n",
    "for i in range(len(datainterpolate)):\n",
    "    final_phase = final_phase * datainterpolate[i][\"phases\"]\n",
    "    datainterpolate[i][\"phases\"] = final_phase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the interpolation and search of the phase vector was successfull and we can now proceed to correct our data. You can find more details about the phase correction algorithm in Ref. 1. \n",
    "For all other calculations that required interpolation, we will for now assume all phasevectors have been found and are (1,1).\n",
    "\n",
    "## Phase correcting data\n",
    "\n",
    "In this subsection, we will now use the found phasevectors to correct our data.\n",
    "We will assume we have found all phasevectors (they are set to +1 for every calculation where we could not found phases for now). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct interpolated geometries\n",
    "\n",
    "Note that for interpolated geometries we need to consecutively multiply the phase vector with every previous phase vector. We have already done this for the interpolated data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the function to correct phases from SchNarc\n",
    "\n",
    "from schnarc.utils import correct_phases\n",
    "\n",
    "# we give the uncorrected data and number of singlets and triplets to the function\n",
    "corrected_data_interpolate = correct_phases(datainterpolate,nsinglets,ntriplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 points could be corected from the original data.\n"
     ]
    }
   ],
   "source": [
    "# correct the rest of the data for which we have phasevectors found\n",
    "\n",
    "corrected_data = correct_phases(data,nsinglets,ntriplets)\n",
    "print(len(corrected_data), \"points could be corected from the original data.\")\n",
    "\n",
    "# get relevant geometries\n",
    "\n",
    "corrected_atoms = []\n",
    "for i in range(len(atoms)):\n",
    "    if \"phases\" not in data[i]:\n",
    "        pass\n",
    "    else:\n",
    "        corrected_atoms.append(atoms[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading a trajectory file from SHARC dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read a trajectory file from SHARC, namely output.dat, make sure you already have converted the data (usually saved in netcdf format) to output.dat and output.xyz files. This can be data via the data-extractor file available with SHARC. Please refer to the SHARC tutorial for more information.\n",
    "\n",
    "For reading such files, we need to specify the number of states and the threshold (0.5) for reliable overlaps. The rest will be read from the output.dat header.\n",
    "Again, we need to give the path of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2410 number of data points found.\n"
     ]
    }
   ],
   "source": [
    "from schnarc.utils import read_traj\n",
    "\n",
    "traj_data = {}\n",
    "traj_geoms = []\n",
    "\n",
    "#we iterate over all files in 10 different trajectory folders,\n",
    "#hence ntraj (number of trajectories) = 10\n",
    "ntraj=10\n",
    "\n",
    "idata = -1\n",
    "for itraj in range((ntraj)):\n",
    "    \n",
    "    path = \"./Trajectories/TRAJ_%05d/\" %(itraj+1)\n",
    "    datatraj_,atomstraj_ = read_traj(path,nsinglets,ntriplets,0.5)\n",
    "    #put data into one large dictionary\n",
    "    for i in range(len(datatraj_)):\n",
    "        idata+=1\n",
    "        traj_data[idata] = datatraj_[i]\n",
    "        traj_geoms.append(ase.atoms.Atoms(atomstraj_[i].get_atomic_numbers(),atomstraj_[i].get_positions()/Bohr))\n",
    "\n",
    "print(len(traj_data), \"number of data points found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can phase correct the data using the function from above if needed by parsing the traj_data data base instead of the previous data base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the data set \n",
    "\n",
    "The data set will be saved in ase format (https://wiki.fysik.dtu.dk/ase/ase/db/db.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase.db import connect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without phase correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have  100  data points.\n"
     ]
    }
   ],
   "source": [
    "# we will only write the necessary properties into the data file\n",
    "\n",
    "keys = [\"energy\", \"nacs\", \"forces\", \"dipoles\", \"has_forces\", \"socs\"]\n",
    "newdata = {}\n",
    "for i in range(len(data)):\n",
    "    newdata[i]={}\n",
    "    for key in keys:\n",
    "        if key in data[i]:\n",
    "            newdata[i][key] = data[i][key]\n",
    "# define the name of the data set\n",
    "#delete any DB that might be here\n",
    "os.system(\"rm -f ../DBs/Fulvene.db\")\n",
    "dbname = \"../DBs/Fulvene.db\"\n",
    "db = connect(dbname)\n",
    "for i in range(len(data)):\n",
    "    #the data are in the \"data\" dicitonary and the atoms saved as atoms objects in the \"atoms\"-array\n",
    "    db.write(atoms[i],data=newdata[i])\n",
    "    \n",
    "#define metadata\n",
    "metadata = {}\n",
    "metadata[\"info\"]=\"Write down any information you wish to remember later, e.g., reference method, if phasecorrected, etc.\"\n",
    "\n",
    "# this information is required\n",
    "metadata[\"n_singlets\"]=nsinglets\n",
    "metadata[\"n_triplets\"]=ntriplets\n",
    "db.metadata=metadata\n",
    "print(\"We have \", len(db), \" data points.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trajectory data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have  2410  traj_data points.\n"
     ]
    }
   ],
   "source": [
    "# we will only write the necessary properties into the traj_data file\n",
    "\n",
    "keys = [\"energy\", \"nacs\", \"forces\", \"dipoles\", \"has_forces\", \"socs\"]\n",
    "newtraj_data = {}\n",
    "for i in range(len(traj_data)):\n",
    "    newtraj_data[i]={}\n",
    "    for key in keys:\n",
    "        if key in traj_data[i]:\n",
    "            newtraj_data[i][key] = traj_data[i][key]\n",
    "# define the name of the traj_data set\n",
    "#delete any DB that might be here\n",
    "os.system(\"rm -f ../DBs/Fulvene_traj.db\")\n",
    "dbname = \"../DBs/Fulvene_traj.db\"\n",
    "db = connect(dbname)\n",
    "for i in range(len(traj_data)):\n",
    "    #the traj_data are in the \"traj_data\" dicitonary and the atoms saved as atoms objects in the \"atoms\"-array\n",
    "    db.write(traj_geoms[i],data=newtraj_data[i])\n",
    "    \n",
    "db.metadata=metadata\n",
    "print(\"We have \", len(db), \" data points.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase corrected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have  32  data points.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dbname = \"../DBs/Fulvene_phasecorrected.db\"\n",
    "os.system(\"rm -f %s\"%dbname)\n",
    "db = connect(dbname)\n",
    "\n",
    "# we will only write the necessary properties into the data file\n",
    "\n",
    "keys = [\"energy\", \"nacs\", \"forces\", \"dipoles\", \"has_forces\", \"socs\"]\n",
    "newdata_corrected = {}\n",
    "for i in range(len(corrected_data)):\n",
    "    newdata_corrected[i]={}\n",
    "    for key in keys:\n",
    "        if key in corrected_data[i]:\n",
    "            newdata_corrected[i][key] = corrected_data[i][key]\n",
    "# we will only write the necessary properties into the data file\n",
    "\n",
    "newdata_corrected_interpolate = {}\n",
    "for i in range(len(corrected_data_interpolate)):\n",
    "    newdata_corrected_interpolate[i]={}\n",
    "    for key in keys:\n",
    "        if key in corrected_data_interpolate:\n",
    "            newdata_corrected_interpolate[i][key] = corrected_data_interpolate[i][key]\n",
    "\n",
    "\n",
    "for i in range(len(corrected_data_interpolate)):\n",
    "    db.write(geoms[i],data=newdata_corrected_interpolate[i])\n",
    "#now write the other corrected data\n",
    "# note that the db will not be overwritten but data will be added with the write function\n",
    "\n",
    "for i in range(len(corrected_data)):\n",
    "    db.write(corrected_atoms[i],data=newdata_corrected[i])\n",
    "\n",
    "# again define the metadata\n",
    "metadata = {}\n",
    "metadata[\"info\"]=\"Write down any information you wish to remember later, e.g., reference method, if phasecorrected, etc.\"\n",
    "\n",
    "# this information is required\n",
    "metadata[\"n_singlets\"]=nsinglets\n",
    "metadata[\"n_triplets\"]=ntriplets\n",
    "db.metadata=metadata\n",
    "print(\"We have \", len(db), \" data points.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have generated an initial training set for SchNarc and can now use it to train your models.\n",
    "\n",
    "Note that if you want to expand the training set you can use the same functions, but make sure to not delete the data base in advance. See the next line to write the same data to the data base again and check the length of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have  300  data points.\n"
     ]
    }
   ],
   "source": [
    "# we will only write the necessary properties into the data file\n",
    "\n",
    "keys = [\"energy\", \"nacs\", \"forces\", \"dipoles\", \"has_forces\", \"socs\"]\n",
    "newdata = {}\n",
    "for i in range(len(data)):\n",
    "    newdata[i]={}\n",
    "    for key in keys:\n",
    "        if key in data[i]:\n",
    "            newdata[i][key] = data[i][key]\n",
    "# define the name of the data set\n",
    "#delete any DB that might be here\n",
    "dbname = \"../DBs/Fulvene.db\"\n",
    "db = connect(dbname)\n",
    "for i in range(len(data)):\n",
    "    #the data are in the \"data\" dicitonary and the atoms saved as atoms objects in the \"atoms\"-array\n",
    "    db.write(atoms[i],data=newdata[i])\n",
    "    \n",
    "#define metadata\n",
    "metadata = {}\n",
    "metadata[\"info\"]=\"Write down any information you wish to remember later, e.g., reference method, if phasecorrected, etc.\"\n",
    "\n",
    "# this information is required\n",
    "metadata[\"n_singlets\"]=nsinglets\n",
    "metadata[\"n_triplets\"]=ntriplets\n",
    "db.metadata=metadata\n",
    "print(\"We have \", len(db), \" data points.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
